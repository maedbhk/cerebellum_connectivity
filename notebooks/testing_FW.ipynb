{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing implementation of forward step-wise regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "import connectivity.evaluation as ev # will be used in stepwise regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creaete a toy example\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y, coef = make_regression(n_samples=92, n_features=20, n_targets=5,\n",
    "                                      n_informative=5, noise=10,\n",
    "                                      coef=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMixin:\n",
    "    \"\"\"\n",
    "    This is a class that can give use extra behaviors or functions that we want our connectivity models to have - over an above the basic functionality provided by the stanard SK-learn BaseEstimator classes\n",
    "    As an example here is a function that serializes the fitted model\n",
    "    Not used right now, but maybe potentially useful. Note that Mixin classes do not have Constructor!\n",
    "    \"\"\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        data = {\"coef_\": self.coef_}\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "class WNTA(ModelMixin):\n",
    "\n",
    "    def __init__(self, n_max, alpha):\n",
    "        self.n_max = n_max # maximum number of features selected\n",
    "        self.alpha = alpha # parameter used in ridge regression\n",
    "\n",
    "    class WNiTA(Ridge, ModelMixin):\n",
    "        def __init__(self, n = 1, alpha = 1):\n",
    "            self.n = n\n",
    "            super(Ridge, self).__init__(alpha = alpha, fit_intercept=False)\n",
    "      \n",
    "        def _add_features(self, X, Y):\n",
    "            \"\"\"\n",
    "            1. start with evaluation of individual features\n",
    "            2. select the one feature that results in the best performance\n",
    "                ** what is the best? That depends on the selected evaluation criteria (in this case it can be R)\n",
    "            3. Consider all the possible combinations of the selected feature and another feature and select the best combination\n",
    "            4. Repeat 1 to 3 untill you have the desired number of features\n",
    "\n",
    "            Args: \n",
    "            X(np.ndarray)   -    design matrix   \n",
    "            Y(np.ndarray)   -    response variables\n",
    "            n(int)          -    number of features to select\n",
    "            \"\"\"\n",
    "\n",
    "            remaining = list(set(range(X.shape[1])) - set(self.selected)) #list containing features that are to be examined\n",
    "\n",
    "            # 2. loop over features\n",
    "            while (remaining) and (len(self.selected)< self.n): # while remaining is not empty and n features are not selected \n",
    "                scores = pd.Series(np.empty((len(remaining))), index=remaining) # the scores will be stored in this \n",
    "                for i in remaining:\n",
    "            \n",
    "                    feats = self.selected +[i] # list containing the current features that will be used in regression\n",
    "                    # fit the model\n",
    "                    ## get the features from X\n",
    "                    X_feat = X[:, list(feats)]\n",
    "\n",
    "                    ## scale X_feat\n",
    "                    scale_ = np.sqrt(np.nansum(X_feat ** 2, 0) / X_feat.shape[0])\n",
    "                    Xs = X_feat / scale_\n",
    "                    Xs = np.nan_to_num(Xs) # there are 0 values after scaling\n",
    "\n",
    "                    ## fit the model\n",
    "                    mod = LinearRegression(fit_intercept=False).fit(Xs, Y)\n",
    "\n",
    "                    ## get the score and put it in scores\n",
    "                    scores.loc[i] = mod.score(Xs, Y, sample_weight=None)\n",
    "\n",
    "                # find the feature/feature combination with the best score\n",
    "                best = scores.idxmax()\n",
    "                self.selected.append(best)\n",
    "                # update remaining\n",
    "                ## remove the selected feature/features from remaining\n",
    "                remaining.remove(best)\n",
    "\n",
    "            return \n",
    "\n",
    "        def select_features(self, X, Y, support_):\n",
    "            \"\"\"\n",
    "            loops over voxels and select features\n",
    "            \"\"\"\n",
    "            self.support_ = np.zeros((Y.shape[1], X.shape[1]))\n",
    "            for vox in range(Y.shape[1]):\n",
    "\n",
    "                # first use self.support_ to get the features already selected \n",
    "                self.selected = list(np.where(support_[vox, :] == 1)[0])\n",
    "\n",
    "                self._add_features(X, Y[:, vox])\n",
    "\n",
    "                self.support_[vox, self.selected] = int(1)\n",
    "\n",
    "            return self.support_\n",
    "\n",
    "        def _fit(self, X, Y, support):\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # get the scaling\n",
    "            self.scale_ = np.sqrt(np.nansum(X ** 2, 0) / X.shape[0])\n",
    "            wnta_coef = np.zeros((Y.shape[1], X.shape[1]))\n",
    "            for vox in range(Y.shape[1]):\n",
    "                # use ModelN.support_ to fit a regression with the selected features\n",
    "                support_vox = support[vox, :]\n",
    "                # get the selected features for the voxel\n",
    "                selected_vox = list(np.where(support_vox == 1)[0])\n",
    "\n",
    "                # get the selected features\n",
    "                X_selected = X[:, selected_vox]\n",
    "\n",
    "                ## scale X_feat\n",
    "                scale_ = np.sqrt(np.nansum(X_selected ** 2, 0) / X_selected.shape[0])\n",
    "                Xs = X_selected / scale_\n",
    "                Xs = np.nan_to_num(Xs) # there are 0 values after scaling\n",
    "\n",
    "                # scale it\n",
    "                Xs = X_selected\n",
    "                super(Ridge, self).fit(Xs, Y[:, vox])\n",
    "\n",
    "                wnta_coef[vox, selected_vox] = self.coef_\n",
    "\n",
    "            self.coef_ = wnta_coef\n",
    "\n",
    "        def _predict(self, X):\n",
    "            Xs = X / self.scale_\n",
    "            Xs = np.nan_to_num(Xs) # there are 0 values after scaling\n",
    "            return Xs @ self.coef_.T  # weights need to be transposed (throws error otherwise)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.models = dict()\n",
    "        support_updated = np.zeros((Y.shape[1], X.shape[1]))\n",
    "        for n in range(1, self.n_max+1):\n",
    "            self.Model = self.WN_1TA(n = n, alpha = self.alpha)\n",
    "            support_updated = self.Model.select_features(X, Y, support_=support_updated) # this creates a numpy array with 1s for the selected features and 0s otherwise\n",
    "\n",
    "            # fit_ridge\n",
    "            self.Model._fit(X, Y, support_updated)\n",
    "\n",
    "            self.models[n] = self.Model\n",
    "\n",
    "    def predict(self):\n",
    "        self.pred = dict()\n",
    "        for n in range(self.n_max):\n",
    "            self.pred[n] = self.models[n]._predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: WN_1TA(), 2: WN_1TA(n=2), 3: WN_1TA(n=3), 4: WN_1TA(n=4), 5: WN_1TA(n=5)}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel = WNTA(n_max=5, alpha = 1)\n",
    "\n",
    "myModel.fit(X, y)\n",
    "myModel.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_add_features', '_check_n_features', '_decision_function', '_estimator_type', '_fit', '_get_param_names', '_get_tags', '_more_tags', '_predict', '_preprocess_data', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_set_intercept', '_validate_data', 'alpha', 'coef_', 'copy_X', 'fit', 'fit_intercept', 'get_params', 'intercept_', 'max_iter', 'n', 'n_features_in_', 'n_iter_', 'normalize', 'predict', 'random_state', 'scale_', 'score', 'select_features', 'selected', 'set_params', 'solver', 'support_', 'to_dict', 'tol']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = myModel.models[1]\n",
    "# print(dir(a))\n",
    "print(a.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_devel(X, Y, n = 1, selected = []):\n",
    "    \"\"\"\n",
    "    1. start with evaluation of individual features\n",
    "    2. select the one feature that results in the best performance\n",
    "        ** what is the best? That depends on the selected evaluation criteria (in this case it can be R)\n",
    "    3. Consider all the possible combinations of the selected feature and another feature and select the best combination\n",
    "    4. Repeat 1 to 3 untill you have the desired number of features\n",
    "\n",
    "    Args: \n",
    "    X(np.ndarray)   -    design matrix   \n",
    "    Y(np.ndarray)   -    response variables\n",
    "    n(int)          -    number of features to select\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. starting with an empty list: the list will be filled with best features eventual\n",
    "    # selected = []\n",
    "    # print(selected)\n",
    "    remaining = list(range(X.shape[1]))\n",
    "\n",
    "    # 2. loop over features\n",
    "    while (remaining) and (len(selected)< n): # while remaining is not empty and n features are not selected \n",
    "        # print(\"looping\")\n",
    "        scores = pd.Series(np.empty((len(remaining))), index=remaining) # the scores will be stored in this \n",
    "        for i in remaining:\n",
    "    \n",
    "            feats = selected +[i] # list containing the current features that will be used in regression\n",
    "            # fit the model\n",
    "            ## get the features from X\n",
    "            X_feat = X[:, list(feats)]\n",
    "\n",
    "            ## scale X_feat\n",
    "            scale_ = np.sqrt(np.nansum(X_feat ** 2, 0) / X_feat.shape[0])\n",
    "            Xs = X_feat / scale_\n",
    "            Xs = np.nan_to_num(Xs) # there are 0 values after scaling\n",
    "\n",
    "            ## fit the model\n",
    "            model = LinearRegression(fit_intercept=False).fit(Xs, Y)\n",
    "\n",
    "            ## get the score and put it in scores\n",
    "            # print(y.shape)\n",
    "            scores.loc[i] = model.score(Xs, Y, sample_weight=None)\n",
    "\n",
    "        # find the feature/feature combination with the best score\n",
    "        best = scores.idxmax()\n",
    "        selected.append(best)\n",
    "        # update remaining\n",
    "        ## remove the selected feature/features from remaining\n",
    "        remaining.remove(best)\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, feature_mask = [], alpha = 2, n = 1):\n",
    "        \"\"\"\n",
    "        feature_mask is a numpy array (#cerebellar voxel-by-#cortical parcel)\n",
    "        with 1s for the selected feature for each voxel and 0s otherwise\n",
    "        the default is an empty list which will be set in the fit routine\n",
    "        \"\"\"\n",
    "        # get the scaling\n",
    "        scale_ = np.sqrt(np.nansum(X ** 2, 0) / X.shape[0])\n",
    "\n",
    "        # looping over cerebellar voxels\n",
    "        num_vox = Y.shape[1]\n",
    "        wnta_coef = np.zeros((Y.shape[1], X.shape[1]))\n",
    "\n",
    "        if not feature_mask: # if the mask is empty, initialize it to be all zeros\n",
    "            feature_mask = np.zeros((Y.shape[1], X.shape[1]))\n",
    "\n",
    "        for vox in range(num_vox):\n",
    "            print(f\"for vox {vox}\")\n",
    "            selected = []\n",
    "            # print(f\"initial selected {selected}\")\n",
    "            # print(f\"vox {vox}\")\n",
    "            # print(f\"{vox}.\", end = \"\", flush = True)\n",
    "            ## get current voxel \n",
    "            y = Y[:, vox]\n",
    "\n",
    "            if np.any(y): # there are voxels with all zeros. Those voxels are skipped and the corresponding coef will be 0\n",
    "                ## use forward selection method to get the best features for each cerebellar voxel\n",
    "                # get the selected features for each cerebellar voxel based off of feature_mask\n",
    "                # print(selected)\n",
    "                if selected:\n",
    "                    # print(\"here\")\n",
    "                    selected = np.argwhere(feature_mask[vox, :] == 1)[0] # get the selected features\n",
    "                    \n",
    "                else:\n",
    "                    # print(\"here2\")\n",
    "                    selected = []\n",
    "\n",
    "                # print(n)\n",
    "                # print(selected)\n",
    "                a = forward_devel(X, y, selected = selected, n = n)\n",
    "                print(f\"selected: {a}\")\n",
    "                # print(f\"a {a}\")\n",
    "                # print(f\"len a {len(a)}\")\n",
    "                # print(f\"selected before: {selected}\")\n",
    "                \n",
    "                selected.append(a[0])\n",
    "                # print(selected)\n",
    "                # print(f\"len selected after {len(selected)}\")\n",
    "                # print(f\"selected after: {selected}\")\n",
    "                \n",
    "                # update the feature mask\n",
    "                feature_mask[vox, selected] = 1\n",
    "\n",
    "                ## use the selected featuers to do a ridge regression \n",
    "                X_selected = X[:, selected]\n",
    "\n",
    "                ### scale X_selected\n",
    "                scale_ = np.sqrt(np.nansum(X_selected ** 2, 0) / X_selected.shape[0])\n",
    "                Xs = X_selected / scale_\n",
    "                Xs = np.nan_to_num(Xs) # there are 0 values after scaling\n",
    "\n",
    "                # print(f\"doing ridge regression\")\n",
    "                model = Ridge(alpha = np.exp(alpha), fit_intercept=False)\n",
    "                model.fit(Xs, y)\n",
    "\n",
    "                # fill in the elements of the coef\n",
    "                wnta_coef[vox, selected] = model.coef_\n",
    "\n",
    "        # self.t1 = time.time()\n",
    "        # self.t1 = time.ctime(self.t1)\n",
    "        # print(f\"\\nfitting finished at {self.t1}\")\n",
    "        # print(f\"fitting took {self.t1 - self.t0} seconds\")\n",
    "        model.coef_ = wnta_coef\n",
    "\n",
    "        return model.coef_, feature_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_scikit(X, Y, n):\n",
    "    \"\"\"\n",
    "    uses scikit learn SequentialFeatureSelector to select the best features\n",
    "\n",
    "    \"\"\"\n",
    "    selector = SequentialFeatureSelector(LinearRegression(fit_intercept=False), n_features_to_select=n)\n",
    "    \n",
    "    # scale X\n",
    "    scale_ = np.sqrt(np.nansum(X ** 2, 0) / X.shape[0])\n",
    "    Xs = X / scale_\n",
    "    Xs = np.nan_to_num(Xs) # there are 0 values after scaling\n",
    "\n",
    "    selector.fit(Xs, Y)\n",
    "\n",
    "    selected_bool = selector.get_support()\n",
    "    selected = np.where(selected_bool)[0]\n",
    "\n",
    "\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMixin:\n",
    "    \"\"\"\n",
    "    This is a class that can give use extra behaviors or functions that we want our connectivity models to have - over an above the basic functionality provided by the stanard SK-learn BaseEstimator classes\n",
    "    As an example here is a function that serializes the fitted model\n",
    "    Not used right now, but maybe potentially useful. Note that Mixin classes do not have Constructor!\n",
    "    \"\"\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        data = {\"coef_\": self.coef_}\n",
    "        return data\n",
    "\n",
    "class WINNERS(ModelMixin):\n",
    "\n",
    "    def __init__(self, n_features_to_select = 1):\n",
    "        self.n_featrues_to_select = n_features_to_select\n",
    "        \n",
    "    def add_features(self, X, y, selected = []):\n",
    "       \n",
    "        \"\"\"\n",
    "        1. start with evaluation of individual features\n",
    "        2. select the one feature that results in the best performance\n",
    "            ** what is the best? That depends on the selected evaluation criteria (in this case it can be R)\n",
    "        3. Consider all the possible combinations of the selected feature and another feature and select the best combination\n",
    "        4. Repeat 1 to 3 untill you have the desired number of features\n",
    "\n",
    "        Args: \n",
    "        X(np.ndarray)   -    design matrix   \n",
    "        Y(np.ndarray)   -    response variables\n",
    "        n(int)          -    number of features to select\n",
    "        \"\"\"\n",
    "        print(f\"the winner class n_features_to_select {self.n_featrues_to_select}\")\n",
    "        remaining = list(set(range(X.shape[1])) - set(selected)) #list containing features that are to be examined\n",
    "\n",
    "        # 2. loop over features\n",
    "        loop = True\n",
    "        while (remaining) and (len(selected) < self.n_featrues_to_select): # while remaining is not empty and n features are not selected \n",
    "            \n",
    "            scores = pd.Series(np.empty((len(remaining))), index=remaining) # the scores will be stored in this \n",
    "            for i in remaining:\n",
    "        \n",
    "                candidates = selected +[i] # list containing the current features that will be used in regression\n",
    "                # fit the model\n",
    "                ## get the features from X\n",
    "                X_feat = X[:, list(candidates)]\n",
    "\n",
    "                ## scale X_feat\n",
    "                scale_ = np.sqrt(np.nansum(X_feat ** 2, 0) / X_feat.shape[0])\n",
    "                Xs = X_feat / scale_\n",
    "                Xs = np.nan_to_num(Xs) # there are 0 values after scaling\n",
    "\n",
    "                ## fit the model\n",
    "                mod = LinearRegression(fit_intercept=False).fit(Xs, y)\n",
    "\n",
    "                ## get the score and put it in scores\n",
    "                ## get the score\n",
    "                score_i, _    = ev.calculate_R(y, mod.predict(Xs))\n",
    "                scores.loc[i] = score_i\n",
    "                                \n",
    "\n",
    "            # find the feature/feature combination with the best score\n",
    "            best       = scores.idxmax()\n",
    "            selected.append(best)\n",
    "\n",
    "            if len(selected) == self.n_featrues_to_select:\n",
    "                print(\"breaking the thing\")\n",
    "                break\n",
    "            # update remaining\n",
    "            ## remove the selected feature/features from remaining\n",
    "            remaining.remove(best)\n",
    "\n",
    "        return selected\n",
    "\n",
    "    def set_support_(self, X, Y, support_ = None):\n",
    "        \"\"\"\n",
    "        gets the support (and updates it) for all the voxels\n",
    "        support_ can then be used to get the selected features\n",
    "        Ars:\n",
    "            X(ndarray)          : contains regressors (cortical regions)\n",
    "            Y(ndarray)          : contains responses (cerebellar voxels)\n",
    "            support_(ndarray)   : initial mask to select features. None: starts from scratch with all zeros\n",
    "        \"\"\"\n",
    "\n",
    "        if support_ is None:\n",
    "            # starting from scratch\n",
    "            self.support_= np.zeros((Y.shape[1], X.shape[1]))\n",
    "            scores_init = []\n",
    "        else:\n",
    "            self.support_ = support_\n",
    "            print(self.n_featrues_to_select)\n",
    "\n",
    "        # loop over voxels\n",
    "        for vox in range(Y.shape[1]):\n",
    "\n",
    "            # get the selected features for the current voxel\n",
    "            initial_feats = list(np.where(self.support_[vox, :] == 1)[0])\n",
    "\n",
    "            # add features to the selected set\n",
    "            feats = self.add_features(X, Y[:, vox], selected = initial_feats)\n",
    "\n",
    "            print(f\"selected features {feats}\")\n",
    "\n",
    "            # update support \n",
    "            self.support_[vox, feats] = int(1)\n",
    "\n",
    "        return self.support_\n",
    "        \n",
    "class WNTA(Ridge, ModelMixin):\n",
    "\n",
    "    def __init__(self, winner_model = None, alpha = 0, positive = False, n_features_to_select = 1):\n",
    "        \"\"\"\n",
    "        should be initialized with an instance of WINNERS class. \n",
    "        if None is entered, it will start from scratch, create an instance of WINNERS \n",
    "        and get the support_ for selecting features. Otherwise, It uses the support_ attribute\n",
    "        of the WINNERS class\n",
    "        \"\"\"\n",
    "\n",
    "        super(Ridge, self).__init__(fit_intercept=False, alpha = alpha)\n",
    "\n",
    "        if winner_model is None:\n",
    "            # initialize a winner model class\n",
    "            self.winner = WINNERS(n_features_to_select = n_features_to_select)\n",
    "        else: \n",
    "            self.winner = winner_model\n",
    "            \n",
    "\n",
    "        self.n_features_to_select = n_features_to_select\n",
    "\n",
    "            \n",
    " \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        # get the scaling\n",
    "        self.scale_ = np.sqrt(np.nansum(X ** 2, 0) / X.shape[0])\n",
    "\n",
    "        # first get the winners\n",
    "        if hasattr(self.winner, \"support_\"): # if it has support_ then it's already been done \n",
    "            self.winner.n_featrues_to_select = self.n_features_to_select\n",
    "            self.feature_mask = self.winner.set_support_(X, Y, self.winner.support_)\n",
    "        else: # then it hasn't been done, so do it\n",
    "            self.feature_mask = self.winner.set_support_(X, Y)\n",
    "            \n",
    "        \n",
    "        # loop over voxels and fit ridge\n",
    "        wnta_coef = np.zeros((Y.shape[1], X.shape[1]))\n",
    "        for vox in range(Y.shape[1]):\n",
    "            # get the selected features for the current voxel\n",
    "            selected_vox = list(np.where(self.feature_mask[vox, :] == 1)[0])\n",
    "\n",
    "            ## use the selected featuers to do a ridge regression \n",
    "            X_selected = X[:, selected_vox]\n",
    "\n",
    "            ### scale X_selected\n",
    "            scale_ = np.sqrt(np.nansum(X_selected ** 2, 0) / X_selected.shape[0])\n",
    "            Xs = X_selected / scale_\n",
    "            Xs = np.nan_to_num(Xs) # there are 0 values after scaling\n",
    "\n",
    "            # print(f\"doing ridge regression\")\n",
    "            super(Ridge, self).fit(Xs, Y[:, vox])\n",
    "\n",
    "            # fill in the elements of the coef\n",
    "            wnta_coef[vox, selected_vox] = self.coef_\n",
    "\n",
    "        # set the coef_ attribute\n",
    "        self.coef_ = wnta_coef\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xs = X / self.scale_\n",
    "        Xs = np.nan_to_num(Xs) # there are 0 values after scaling\n",
    "        return Xs @ self.coef_.T  # weights need to be transposed (throws error otherwise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support is None\n",
      "initial set []\n",
      "the winner class n_features_to_select 2\n",
      "len selected 1\n",
      "number of selected 2\n",
      "len selected 2\n",
      "number of selected 2\n",
      "breaking the thing\n",
      "selected features [10, 3]\n",
      "initial set []\n",
      "the winner class n_features_to_select 2\n",
      "len selected 1\n",
      "number of selected 2\n",
      "len selected 2\n",
      "number of selected 2\n",
      "breaking the thing\n",
      "selected features [1, 3]\n",
      "initial set []\n",
      "the winner class n_features_to_select 2\n",
      "len selected 1\n",
      "number of selected 2\n",
      "len selected 2\n",
      "number of selected 2\n",
      "breaking the thing\n",
      "selected features [3, 1]\n",
      "initial set []\n",
      "the winner class n_features_to_select 2\n",
      "len selected 1\n",
      "number of selected 2\n",
      "len selected 2\n",
      "number of selected 2\n",
      "breaking the thing\n",
      "selected features [2, 4]\n",
      "initial set []\n",
      "the winner class n_features_to_select 2\n",
      "len selected 1\n",
      "number of selected 2\n",
      "len selected 2\n",
      "number of selected 2\n",
      "breaking the thing\n",
      "selected features [2, 4]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "MModel = WINNERS(n_features_to_select=2)\n",
    "MModel.set_support_(X, y)\n",
    "\n",
    "print(MModel.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting from scratch\n",
      "calculating support_\n",
      "support is None\n",
      "initial set []\n",
      "the winner class n_features_to_select 3\n",
      "len selected 1\n",
      "number of selected 3\n",
      "len selected 2\n",
      "number of selected 3\n",
      "len selected 3\n",
      "number of selected 3\n",
      "breaking the thing\n",
      "selected features [10, 3, 4]\n",
      "initial set []\n",
      "the winner class n_features_to_select 3\n",
      "len selected 1\n",
      "number of selected 3\n",
      "len selected 2\n",
      "number of selected 3\n",
      "len selected 3\n",
      "number of selected 3\n",
      "breaking the thing\n",
      "selected features [1, 3, 4]\n",
      "initial set []\n",
      "the winner class n_features_to_select 3\n",
      "len selected 1\n",
      "number of selected 3\n",
      "len selected 2\n",
      "number of selected 3\n",
      "len selected 3\n",
      "number of selected 3\n",
      "breaking the thing\n",
      "selected features [3, 1, 10]\n",
      "initial set []\n",
      "the winner class n_features_to_select 3\n",
      "len selected 1\n",
      "number of selected 3\n",
      "len selected 2\n",
      "number of selected 3\n",
      "len selected 3\n",
      "number of selected 3\n",
      "breaking the thing\n",
      "selected features [2, 4, 3]\n",
      "initial set []\n",
      "the winner class n_features_to_select 3\n",
      "len selected 1\n",
      "number of selected 3\n",
      "len selected 2\n",
      "number of selected 3\n",
      "len selected 3\n",
      "number of selected 3\n",
      "breaking the thing\n",
      "selected features [2, 4, 3]\n",
      "(5, 20)\n",
      "[3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "Model2 = WNTA(n_features_to_select= 3)\n",
    "Model2.fit(X, y)\n",
    "print(Model2.coef_.shape)\n",
    "print(np.sum(Model2.coef_ != 0, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the model\n",
      "support_ already calculated\n",
      "support is not None\n",
      "3\n",
      "initial set [3, 10]\n",
      "the winner class n_features_to_select 3\n",
      "len selected 3\n",
      "number of selected 3\n",
      "breaking the thing\n",
      "selected features [3, 10, 4]\n",
      "initial set [1, 3]\n",
      "the winner class n_features_to_select 3\n",
      "len selected 3\n",
      "number of selected 3\n",
      "breaking the thing\n",
      "selected features [1, 3, 4]\n",
      "initial set [1, 3]\n",
      "the winner class n_features_to_select 3\n",
      "len selected 3\n",
      "number of selected 3\n",
      "breaking the thing\n",
      "selected features [1, 3, 10]\n",
      "initial set [2, 4]\n",
      "the winner class n_features_to_select 3\n",
      "len selected 3\n",
      "number of selected 3\n",
      "breaking the thing\n",
      "selected features [2, 4, 3]\n",
      "initial set [2, 4]\n",
      "the winner class n_features_to_select 3\n",
      "len selected 3\n",
      "number of selected 3\n",
      "breaking the thing\n",
      "selected features [2, 4, 3]\n",
      "(5, 20)\n",
      "[3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "Model3 = WNTA(n_features_to_select= 3, winner_model=MModel)\n",
    "Model3.fit(X, y)\n",
    "print(Model3.coef_.shape)\n",
    "print(np.sum(Model3.coef_ != 0, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(Model2.coef_, Model3.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('cerebellum_connectivity-pszUI8aN': pipenv)",
   "name": "connectivity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}